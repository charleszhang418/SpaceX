{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13550,"status":"ok","timestamp":1696774651493,"user":{"displayName":"Charles Zhang","userId":"10373813694973136079"},"user_tz":240},"id":"PpvupVpsvkXr","outputId":"cd6a751d-cb6f-44c4-d36b-bc1d96c904f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["# Note, run this only once, all the following based on directory structure /content/gdrive/\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":327,"status":"ok","timestamp":1696774790621,"user":{"displayName":"Charles Zhang","userId":"10373813694973136079"},"user_tz":240},"id":"9fHEHnd3vxYs","outputId":"c21e15a2-be9d-4853-ab08-674d60384b8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/MyDrive/NASA/DNABERT/examples\n","compute_result.py      run_finetune.py\tsample_data\n","data_process_template  run_pretrain.py\tscripts\n","requirements.txt       runs\t\tvisualize.py\n"]}],"source":["# Run this only once into your DNABERT directory\n","%cd gdrive/MyDrive/NASA/DNABERT\n","\n","!ls"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34502,"status":"ok","timestamp":1696774745779,"user":{"displayName":"Charles Zhang","userId":"10373813694973136079"},"user_tz":240},"id":"Vdl1xP02v0BC","outputId":"b86d3c34-78da-4926-a26d-6b7e8d0da8aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n","Collecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n","Collecting dill (from evaluate)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n","Collecting xxhash (from evaluate)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from evaluate)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.17.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Installing collected packages: xxhash, dill, responses, multiprocess, datasets, evaluate\n","Successfully installed datasets-2.14.5 dill-0.3.7 evaluate-0.4.0 multiprocess-0.70.15 responses-0.18.0 xxhash-3.4.1\n"]}],"source":["!pip install transformers\n","!pip install evaluate\n","\n","import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, BertModel\n","from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n","from sklearn.model_selection import train_test_split\n","import evaluate\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"elapsed":1150,"status":"ok","timestamp":1696716580937,"user":{"displayName":"Charles Zhang","userId":"10373813694973136079"},"user_tz":240},"id":"Tq_dICWZv4hA","outputId":"da9f0bcc-8160-4863-853e-304c26f68281"},"outputs":[{"name":"stdout","output_type":"stream","text":["(36000, 5)\n"]},{"data":{"text/html":["\n","  <div id=\"df-bb63316a-6304-42b5-95de-351479295bc8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>notation</th>\n","      <th>DNA</th>\n","      <th>mass</th>\n","      <th>filename</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A00654:48:HN52TDRXY:1:2101:5990:1000 2:N:0:AGT...</td>\n","      <td>ATATTTATGGCTGGACTTGAACTTACTAAGTAGACCATGCTGGCCT...</td>\n","      <td>FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF...</td>\n","      <td>GLDS-466_metagenomics_RR10_FCS_VIV_WT_V1_R2_HR...</td>\n","      <td>Wild Type</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A00654:48:HN52TDRXY:1:2101:6840:1000 2:N:0:AGT...</td>\n","      <td>CCTACGCTCAGCGAGGCGACTTTGAGAGATGCGCCGAAGAATCTTT...</td>\n","      <td>FFFF:F,F:FFFFFFFFFFF,FFF::FFFFFFFFF,,F,,FFF,F:...</td>\n","      <td>GLDS-466_metagenomics_RR10_FCS_VIV_WT_V1_R2_HR...</td>\n","      <td>Wild Type</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A00654:48:HN52TDRXY:1:2101:9498:1000 2:N:0:AGT...</td>\n","      <td>GCCTTGACCCATGCCTGATAAGGGAGGGCCCGGTCGACGCCCAGGA...</td>\n","      <td>:FFFFFFFFFFFFFFFF:FFFFFFFFFF,FF:FFFFF:FFFFFFFF...</td>\n","      <td>GLDS-466_metagenomics_RR10_FCS_VIV_WT_V1_R2_HR...</td>\n","      <td>Wild Type</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A00654:48:HN52TDRXY:1:2101:15067:1000 2:N:0:AG...</td>\n","      <td>GGACAGGGCCGCAGCATATTCTCATTAAACGGCTGGCCGTCATGGT...</td>\n","      <td>FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF...</td>\n","      <td>GLDS-466_metagenomics_RR10_FCS_VIV_WT_V1_R2_HR...</td>\n","      <td>Wild Type</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A00654:48:HN52TDRXY:1:2101:1298:1016 2:N:0:AGT...</td>\n","      <td>TCCTCCATCCATGCTTTGAATGTCTGGAACCCTGCCTCTGTGTTAC...</td>\n","      <td>FFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFF:F...</td>\n","      <td>GLDS-466_metagenomics_RR10_FCS_VIV_WT_V1_R2_HR...</td>\n","      <td>Wild Type</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb63316a-6304-42b5-95de-351479295bc8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bb63316a-6304-42b5-95de-351479295bc8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bb63316a-6304-42b5-95de-351479295bc8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a08afbd4-3d33-42a3-a8c0-43b01334bdf5\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a08afbd4-3d33-42a3-a8c0-43b01334bdf5')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a08afbd4-3d33-42a3-a8c0-43b01334bdf5 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                            notation  \\\n","0  A00654:48:HN52TDRXY:1:2101:5990:1000 2:N:0:AGT...   \n","1  A00654:48:HN52TDRXY:1:2101:6840:1000 2:N:0:AGT...   \n","2  A00654:48:HN52TDRXY:1:2101:9498:1000 2:N:0:AGT...   \n","3  A00654:48:HN52TDRXY:1:2101:15067:1000 2:N:0:AG...   \n","4  A00654:48:HN52TDRXY:1:2101:1298:1016 2:N:0:AGT...   \n","\n","                                                 DNA  \\\n","0  ATATTTATGGCTGGACTTGAACTTACTAAGTAGACCATGCTGGCCT...   \n","1  CCTACGCTCAGCGAGGCGACTTTGAGAGATGCGCCGAAGAATCTTT...   \n","2  GCCTTGACCCATGCCTGATAAGGGAGGGCCCGGTCGACGCCCAGGA...   \n","3  GGACAGGGCCGCAGCATATTCTCATTAAACGGCTGGCCGTCATGGT...   \n","4  TCCTCCATCCATGCTTTGAATGTCTGGAACCCTGCCTCTGTGTTAC...   \n","\n","                                                mass  \\\n","0  FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF...   \n","1  FFFF:F,F:FFFFFFFFFFF,FFF::FFFFFFFFF,,F,,FFF,F:...   \n","2  :FFFFFFFFFFFFFFFF:FFFFFFFFFF,FF:FFFFF:FFFFFFFF...   \n","3  FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF...   \n","4  FFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFF:F...   \n","\n","                                            filename      label  \n","0  GLDS-466_metagenomics_RR10_FCS_VIV_WT_V1_R2_HR...  Wild Type  \n","1  GLDS-466_metagenomics_RR10_FCS_VIV_WT_V1_R2_HR...  Wild Type  \n","2  GLDS-466_metagenomics_RR10_FCS_VIV_WT_V1_R2_HR...  Wild Type  \n","3  GLDS-466_metagenomics_RR10_FCS_VIV_WT_V1_R2_HR...  Wild Type  \n","4  GLDS-466_metagenomics_RR10_FCS_VIV_WT_V1_R2_HR...  Wild Type  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["dna_data = pd.read_csv('data/dna_data.csv')\n","print(dna_data.shape)\n","dna_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27475,"status":"ok","timestamp":1696727991584,"user":{"displayName":"Charles Zhang","userId":"10373813694973136079"},"user_tz":240},"id":"EQ1DlsnxwE02","outputId":"6e9ba99a-8813-4d6a-e14c-a16d71412e56"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'DNABERT'...\n","remote: Enumerating objects: 774, done.\u001b[K\n","remote: Counting objects: 100% (74/74), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 774 (delta 56), reused 50 (delta 50), pack-reused 700\u001b[K\n","Receiving objects: 100% (774/774), 11.68 MiB | 7.11 MiB/s, done.\n","Resolving deltas: 100% (412/412), done.\n","Updating files: 100% (252/252), done.\n","/content/gdrive/MyDrive/NASA/DNABERT/examples/DNABERT\n","Obtaining file:///content/gdrive/MyDrive/NASA/DNABERT/examples/DNABERT\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==2.5.0) (1.23.5)\n","Collecting tokenizers==0.5.0 (from transformers==2.5.0)\n","  Using cached tokenizers-0.5.0.tar.gz (64 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting boto3 (from transformers==2.5.0)\n","  Using cached boto3-1.28.62-py3-none-any.whl (135 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==2.5.0) (3.12.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==2.5.0) (2.31.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==2.5.0) (4.66.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==2.5.0) (2023.6.3)\n","Collecting sentencepiece (from transformers==2.5.0)\n","  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","Collecting sacremoses (from transformers==2.5.0)\n","  Using cached sacremoses-0.0.53-py3-none-any.whl\n","Collecting botocore<1.32.0,>=1.31.62 (from boto3->transformers==2.5.0)\n","  Using cached botocore-1.31.62-py3-none-any.whl (11.2 MB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3->transformers==2.5.0)\n","  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.8.0,>=0.7.0 (from boto3->transformers==2.5.0)\n","  Using cached s3transfer-0.7.0-py3-none-any.whl (79 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.5.0) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.5.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.5.0) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.5.0) (2023.7.22)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.5.0) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.5.0) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.5.0) (1.3.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.62->boto3->transformers==2.5.0) (2.8.2)\n","Building wheels for collected packages: tokenizers\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0mFailed to build tokenizers\n","\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m/content/gdrive/MyDrive/NASA/DNABERT/examples/DNABERT/examples\n","Collecting tensorboardX (from -r requirements.txt (line 1))\n","  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.13.0)\n","Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.2.2)\n","Collecting seqeval (from -r requirements.txt (line 4))\n","  Using cached seqeval-1.2.2.tar.gz (43 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyahocorasick (from -r requirements.txt (line 5))\n","  Using cached pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.11.3)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.14.0)\n","Collecting biopython (from -r requirements.txt (line 8))\n","  Using cached biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.3)\n","Collecting pybedtools (from -r requirements.txt (line 10))\n","  Using cached pybedtools-0.9.1.tar.gz (12.5 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting sentencepiece==0.1.91 (from -r requirements.txt (line 11))\n","  Using cached sentencepiece-0.1.91.tar.gz (500 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}],"source":["!git clone https://github.com/jerryji1993/DNABERT\n","%cd DNABERT\n","!python3 -m pip install --editable .\n","%cd examples\n","!python3 -m pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"LWlx3fLLwN1t"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: 'examples'\n","/content/gdrive/MyDrive/NASA/DNABERT/examples\n","env: KMER=6\n","env: TRAIN_FILE=./sample_data/pre/dna_str.txt\n","env: TEST_FILE=./sample_data/pre/dna_str.txt\n","env: SOURCE=/content/gdrive/MyDrive/NASA/DNABERT\n","env: OUTPUT_PATH=output$KMER\n","2023-10-08 14:25:35.687153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","10/08/2023 14:25:36 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1946: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n","  warnings.warn(\n","10/08/2023 14:25:36 - INFO - __main__ -   Training new model from scratch\n","10/08/2023 14:25:38 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='./sample_data/pre/dna_str.txt', output_dir='output$KMER', model_type='dna', eval_data_file='./sample_data/pre/dna_str.txt', line_by_line=True, should_continue=False, model_name_or_path=None, mlm=True, mlm_probability=0.025, config_name='/content/gdrive/MyDrive/NASA/DNABERT/src/transformers/dnabert-config/bert-config-6/config.json', tokenizer_name='/content/gdrive/MyDrive/NASA/DNABERT/src/transformers/dnabert-config/bert-config-6/vocab.txt', cache_dir=None, block_size=64, do_train=True, do_eval=True, evaluate_during_training=True, per_gpu_train_batch_size=10, per_gpu_eval_batch_size=6, gradient_accumulation_steps=25, learning_rate=0.0004, weight_decay=0.01, adam_epsilon=1e-06, beta1=0.9, beta2=0.98, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=200000, warmup_steps=10000, logging_steps=500, save_steps=500, save_total_limit=20, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, n_process=24, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=0, device=device(type='cpu'))\n","10/08/2023 14:25:38 - INFO - __main__ -   Creating features from dataset file at ./sample_data/pre/dna_str.txt\n","0 start\n","1 start\n","2 start\n","3 start\n","4 start\n","5 start\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","6 start\n","7 start\n","8 start\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","9 start\n","10 start\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","11 start\n","12 start\n","13 start\n","14 start\n","15 start\n","16 start\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","17 start\n","18 start\n","19 start\n","20 start\n","21 start\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","22 start\n","23 start\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","10/08/2023 14:27:20 - INFO - __main__ -   Saving features into cached file ./sample_data/pre/dna_cached_lm_64_dna_str.txt\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","10/08/2023 14:27:20 - INFO - __main__ -   ***** Running training *****\n","10/08/2023 14:27:20 - INFO - __main__ -     Num examples = 12000\n","10/08/2023 14:27:20 - INFO - __main__ -     Num Epochs = 4167\n","10/08/2023 14:27:20 - INFO - __main__ -     Instantaneous batch size per GPU = 10\n","10/08/2023 14:27:20 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 250\n","10/08/2023 14:27:20 - INFO - __main__ -     Gradient Accumulation steps = 25\n","10/08/2023 14:27:20 - INFO - __main__ -     Total optimization steps = 200000\n","Epoch:   0% 0/4167 [00:00<?, ?it/s]\n","Iteration:   0% 0/1200 [00:00<?, ?it/s]\u001b[A\n","Iteration:   0% 1/1200 [00:07<2:39:36,  7.99s/it]\u001b[A\n","Iteration:   0% 2/1200 [00:14<2:23:46,  7.20s/it]\u001b[A\n","Iteration:   0% 3/1200 [00:20<2:07:15,  6.38s/it]\u001b[A\n","Iteration:   0% 4/1200 [00:27<2:19:04,  6.98s/it]\u001b[A\n","Iteration:   0% 5/1200 [00:33<2:07:11,  6.39s/it]\u001b[A\n","Iteration:   0% 6/1200 [00:39<2:07:59,  6.43s/it]\u001b[A\n","Iteration:   1% 7/1200 [00:46<2:10:03,  6.54s/it]\u001b[A\n","Iteration:   1% 8/1200 [00:51<2:02:58,  6.19s/it]\u001b[A\n","Iteration:   1% 9/1200 [00:59<2:14:00,  6.75s/it]\u001b[A\n","Iteration:   1% 10/1200 [01:05<2:05:57,  6.35s/it]\u001b[A\n","Iteration:   1% 11/1200 [01:11<2:03:32,  6.23s/it]\u001b[A\n","Iteration:   1% 12/1200 [01:18<2:09:58,  6.56s/it]\u001b[A\n","Iteration:   1% 13/1200 [01:24<2:02:24,  6.19s/it]\u001b[A\n","Iteration:   1% 14/1200 [01:31<2:09:33,  6.55s/it]\u001b[A\n","Iteration:   1% 15/1200 [01:37<2:04:58,  6.33s/it]\u001b[A\n","Iteration:   1% 16/1200 [01:42<1:59:04,  6.03s/it]\u001b[A\n","Iteration:   1% 17/1200 [01:50<2:09:54,  6.59s/it]\u001b[A\n","Iteration:   2% 18/1200 [01:56<2:04:35,  6.32s/it]\u001b[A\n","Iteration:   2% 19/1200 [02:03<2:08:19,  6.52s/it]\u001b[A\n","Iteration:   2% 20/1200 [02:09<2:07:20,  6.48s/it]\u001b[A\n","Iteration:   2% 21/1200 [02:14<2:00:45,  6.15s/it]\u001b[A\n","Iteration:   2% 22/1200 [02:22<2:11:02,  6.67s/it]\u001b[A\n","Iteration:   2% 23/1200 [02:28<2:03:10,  6.28s/it]\u001b[A\n","Iteration:   2% 24/1200 [02:34<2:01:37,  6.21s/it]\u001b[A\n","Iteration:   2% 25/1200 [02:42<2:12:30,  6.77s/it]\u001b[A\n","Iteration:   2% 26/1200 [02:47<2:04:17,  6.35s/it]\u001b[A\n","Iteration:   2% 27/1200 [02:55<2:13:26,  6.83s/it]\u001b[A\n","Iteration:   2% 28/1200 [03:01<2:05:06,  6.40s/it]\u001b[A\n","Iteration:   2% 29/1200 [03:07<2:05:20,  6.42s/it]\u001b[A\n","Iteration:   2% 30/1200 [03:14<2:07:49,  6.55s/it]\u001b[A\n","Iteration:   3% 31/1200 [03:19<2:00:48,  6.20s/it]\u001b[A\n","Iteration:   3% 32/1200 [03:27<2:10:15,  6.69s/it]\u001b[A\n","Iteration:   3% 33/1200 [03:32<2:02:28,  6.30s/it]\u001b[A\n","Iteration:   3% 34/1200 [03:38<1:59:16,  6.14s/it]\u001b[A\n","Iteration:   3% 35/1200 [03:46<2:06:33,  6.52s/it]\u001b[A\n","Iteration:   3% 36/1200 [03:51<1:59:56,  6.18s/it]\u001b[A\n","Iteration:   3% 37/1200 [03:58<2:06:44,  6.54s/it]\u001b[A\n","Iteration:   3% 38/1200 [04:04<2:02:51,  6.34s/it]\u001b[A\n","Iteration:   3% 39/1200 [04:10<1:57:16,  6.06s/it]\u001b[A\n","Iteration:   3% 40/1200 [04:18<2:08:12,  6.63s/it]\u001b[A\n","Iteration:   3% 41/1200 [04:23<2:00:49,  6.25s/it]\u001b[A\n","Iteration:   4% 42/1200 [04:30<2:03:54,  6.42s/it]\u001b[A\n","Iteration:   4% 43/1200 [04:36<2:03:30,  6.40s/it]\u001b[A\n","Iteration:   4% 44/1200 [04:42<1:57:23,  6.09s/it]\u001b[A\n","Iteration:   4% 45/1200 [04:49<2:07:55,  6.65s/it]\u001b[A\n","Iteration:   4% 46/1200 [04:55<2:00:52,  6.28s/it]\u001b[A\n","Iteration:   4% 47/1200 [05:01<1:59:55,  6.24s/it]\u001b[A\n","Iteration:   4% 48/1200 [05:08<2:05:11,  6.52s/it]\u001b[A\n","Iteration:   4% 49/1200 [05:14<1:58:42,  6.19s/it]\u001b[A\n","Iteration:   4% 50/1200 [05:22<2:13:13,  6.95s/it]\u001b[A\n","Iteration:   4% 51/1200 [05:28<2:03:39,  6.46s/it]\u001b[A\n","Iteration:   4% 52/1200 [05:34<2:01:03,  6.33s/it]\u001b[A\n","Iteration:   4% 53/1200 [05:41<2:06:04,  6.60s/it]\u001b[A\n","Iteration:   4% 54/1200 [05:46<1:58:42,  6.21s/it]\u001b[A\n","Iteration:   5% 55/1200 [05:54<2:05:17,  6.57s/it]\u001b[A\n","Iteration:   5% 56/1200 [05:59<2:00:41,  6.33s/it]\u001b[A\n","Iteration:   5% 57/1200 [06:05<1:54:38,  6.02s/it]\u001b[A\n","Iteration:   5% 58/1200 [06:13<2:05:14,  6.58s/it]\u001b[A\n","Iteration:   5% 59/1200 [06:18<1:58:24,  6.23s/it]\u001b[A\n","Iteration:   5% 60/1200 [06:25<2:01:41,  6.40s/it]\u001b[A\n","Iteration:   5% 61/1200 [06:31<2:01:43,  6.41s/it]\u001b[A\n","Iteration:   5% 62/1200 [06:37<1:59:04,  6.28s/it]\u001b[A\n","Iteration:   5% 63/1200 [06:46<2:15:17,  7.14s/it]\u001b[A\n","Iteration:   5% 64/1200 [06:52<2:05:08,  6.61s/it]\u001b[A\n","Iteration:   5% 65/1200 [06:58<2:02:39,  6.48s/it]\u001b[A\n","Iteration:   6% 66/1200 [07:05<2:06:04,  6.67s/it]\u001b[A\n","Iteration:   6% 67/1200 [07:10<1:58:32,  6.28s/it]\u001b[A\n","Iteration:   6% 68/1200 [07:18<2:06:38,  6.71s/it]\u001b[A\n","Iteration:   6% 69/1200 [07:24<2:00:49,  6.41s/it]\u001b[A\n","Iteration:   6% 70/1200 [07:29<1:56:22,  6.18s/it]\u001b[A\n","Iteration:   6% 71/1200 [07:37<2:03:55,  6.59s/it]\u001b[A\n","Iteration:   6% 72/1200 [07:42<1:56:49,  6.21s/it]\u001b[A\n","Iteration:   6% 73/1200 [07:49<2:00:57,  6.44s/it]\u001b[A\n","Iteration:   6% 74/1200 [07:55<1:58:49,  6.33s/it]\u001b[A\n","Iteration:   6% 75/1200 [08:02<1:57:33,  6.27s/it]\u001b[A\n","Iteration:   6% 76/1200 [08:09<2:06:14,  6.74s/it]\u001b[A\n","Iteration:   6% 77/1200 [08:15<1:58:41,  6.34s/it]\u001b[A\n","Iteration:   6% 78/1200 [08:22<2:02:10,  6.53s/it]\u001b[A\n","Iteration:   7% 79/1200 [08:28<2:02:04,  6.53s/it]\u001b[A\n","Iteration:   7% 80/1200 [08:34<1:55:24,  6.18s/it]\u001b[A\n","Iteration:   7% 81/1200 [08:42<2:05:07,  6.71s/it]\u001b[A\n","Iteration:   7% 82/1200 [08:47<1:57:37,  6.31s/it]\u001b[A\n","Iteration:   7% 83/1200 [08:53<1:57:39,  6.32s/it]\u001b[A\n","Iteration:   7% 84/1200 [09:00<2:01:04,  6.51s/it]\u001b[A\n","Iteration:   7% 85/1200 [09:06<1:54:38,  6.17s/it]\u001b[A\n","Iteration:   7% 86/1200 [09:13<2:02:31,  6.60s/it]\u001b[A\n","Iteration:   7% 87/1200 [09:19<1:56:41,  6.29s/it]\u001b[A\n","Iteration:   7% 88/1200 [09:24<1:52:47,  6.09s/it]\u001b[A\n","Iteration:   7% 89/1200 [09:32<2:01:35,  6.57s/it]\u001b[A\n","Iteration:   8% 90/1200 [09:37<1:54:25,  6.19s/it]\u001b[A\n","Iteration:   8% 91/1200 [09:44<1:58:51,  6.43s/it]\u001b[A\n","Iteration:   8% 92/1200 [09:51<1:57:10,  6.34s/it]\u001b[A\n","Iteration:   8% 93/1200 [09:56<1:51:27,  6.04s/it]\u001b[A\n","Iteration:   8% 94/1200 [10:04<2:01:40,  6.60s/it]\u001b[A\n","Iteration:   8% 95/1200 [10:09<1:54:35,  6.22s/it]\u001b[A\n","Iteration:   8% 96/1200 [10:15<1:54:52,  6.24s/it]\u001b[A\n","Iteration:   8% 97/1200 [10:22<1:58:42,  6.46s/it]\u001b[A\n","Iteration:   8% 98/1200 [10:28<1:52:25,  6.12s/it]\u001b[A\n","Iteration:   8% 99/1200 [10:35<2:01:00,  6.59s/it]\u001b[A\n","Iteration:   8% 100/1200 [10:42<1:59:47,  6.53s/it]\u001b[A\n","Iteration:   8% 101/1200 [10:48<1:57:12,  6.40s/it]\u001b[A\n","Iteration:   8% 102/1200 [10:55<2:00:51,  6.60s/it]\u001b[A\n","Iteration:   9% 103/1200 [11:00<1:54:00,  6.24s/it]\u001b[A\n","Iteration:   9% 104/1200 [11:08<2:01:26,  6.65s/it]\u001b[A\n","Iteration:   9% 105/1200 [11:14<1:55:52,  6.35s/it]\u001b[A\n","Iteration:   9% 106/1200 [11:19<1:51:06,  6.09s/it]\u001b[A\n","Iteration:   9% 107/1200 [11:27<2:00:35,  6.62s/it]\u001b[A\n","Iteration:   9% 108/1200 [11:32<1:53:41,  6.25s/it]\u001b[A\n","Iteration:   9% 109/1200 [11:39<1:58:01,  6.49s/it]\u001b[A\n","Iteration:   9% 110/1200 [11:46<1:57:58,  6.49s/it]\u001b[A\n","Iteration:   9% 111/1200 [11:51<1:52:29,  6.20s/it]\u001b[A\n","Iteration:   9% 112/1200 [12:00<2:03:01,  6.78s/it]\u001b[A\n","Iteration:   9% 113/1200 [12:05<1:56:45,  6.44s/it]\u001b[A\n","Iteration:  10% 114/1200 [12:13<2:02:08,  6.75s/it]\u001b[A\n","Iteration:  10% 115/1200 [12:19<2:01:30,  6.72s/it]\u001b[A\n","Iteration:  10% 116/1200 [12:25<1:55:31,  6.39s/it]\u001b[A\n","Iteration:  10% 117/1200 [12:33<2:06:01,  6.98s/it]\u001b[A\n","Iteration:  10% 118/1200 [12:39<1:58:48,  6.59s/it]\u001b[A\n","Iteration:  10% 119/1200 [12:46<2:02:45,  6.81s/it]\u001b[A\n","Iteration:  10% 120/1200 [12:53<2:00:41,  6.71s/it]\u001b[A\n","Iteration:  10% 121/1200 [12:58<1:55:04,  6.40s/it]\u001b[A\n","Iteration:  10% 122/1200 [13:07<2:05:14,  6.97s/it]\u001b[A\n","Iteration:  10% 123/1200 [13:12<1:57:17,  6.53s/it]\u001b[A\n","Iteration:  10% 124/1200 [13:20<2:02:18,  6.82s/it]\u001b[A\n","Iteration:  10% 125/1200 [13:27<2:04:39,  6.96s/it]\u001b[A\n","Iteration:  10% 126/1200 [13:33<2:00:30,  6.73s/it]\u001b[A\n","Iteration:  11% 127/1200 [13:41<2:05:37,  7.03s/it]\u001b[A\n","Iteration:  11% 128/1200 [13:47<1:57:39,  6.59s/it]\u001b[A\n","Iteration:  11% 129/1200 [13:54<2:04:05,  6.95s/it]\u001b[A\n","Iteration:  11% 130/1200 [14:00<1:57:05,  6.57s/it]\u001b[A\n","Iteration:  11% 131/1200 [14:06<1:53:20,  6.36s/it]\u001b[A\n","Iteration:  11% 132/1200 [14:13<1:59:39,  6.72s/it]\u001b[A\n","Iteration:  11% 133/1200 [14:19<1:52:30,  6.33s/it]\u001b[A\n","Iteration:  11% 134/1200 [14:26<1:57:43,  6.63s/it]\u001b[A\n","Iteration:  11% 135/1200 [14:32<1:54:32,  6.45s/it]\u001b[A\n","Iteration:  11% 136/1200 [14:38<1:49:17,  6.16s/it]\u001b[A\n","Iteration:  11% 137/1200 [14:46<1:59:07,  6.72s/it]\u001b[A\n","Iteration:  12% 138/1200 [14:51<1:52:17,  6.34s/it]\u001b[A\n","Iteration:  12% 139/1200 [14:58<1:56:55,  6.61s/it]\u001b[A\n","Iteration:  12% 140/1200 [15:05<1:56:50,  6.61s/it]\u001b[A\n","Iteration:  12% 141/1200 [15:11<1:51:03,  6.29s/it]\u001b[A\n","Iteration:  12% 142/1200 [15:19<2:00:42,  6.85s/it]\u001b[A\n","Iteration:  12% 143/1200 [15:24<1:53:51,  6.46s/it]\u001b[A\n","Iteration:  12% 144/1200 [15:31<1:57:01,  6.65s/it]\u001b[A\n","Iteration:  12% 145/1200 [15:38<1:56:31,  6.63s/it]\u001b[A\n","Iteration:  12% 146/1200 [15:43<1:50:36,  6.30s/it]\u001b[A\n","Iteration:  12% 147/1200 [15:52<2:00:43,  6.88s/it]\u001b[A\n","Iteration:  12% 148/1200 [15:57<1:53:49,  6.49s/it]\u001b[A\n","Iteration:  12% 149/1200 [16:04<1:56:13,  6.63s/it]\u001b[A\n","Iteration:  12% 150/1200 [16:12<2:00:21,  6.88s/it]\u001b[A\n","Iteration:  13% 151/1200 [16:17<1:53:21,  6.48s/it]\u001b[A\n","Iteration:  13% 152/1200 [16:25<2:00:49,  6.92s/it]\u001b[A\n","Iteration:  13% 153/1200 [16:31<1:52:57,  6.47s/it]\u001b[A\n","Iteration:  13% 154/1200 [16:38<1:56:33,  6.69s/it]\u001b[A\n","Iteration:  13% 155/1200 [16:44<1:56:13,  6.67s/it]\u001b[A\n","Iteration:  13% 156/1200 [16:50<1:50:39,  6.36s/it]\u001b[A\n","Iteration:  13% 157/1200 [16:58<2:00:40,  6.94s/it]\u001b[A\n","Iteration:  13% 158/1200 [17:04<1:54:03,  6.57s/it]\u001b[A\n","Iteration:  13% 159/1200 [17:11<1:57:44,  6.79s/it]\u001b[A\n","Iteration:  13% 160/1200 [17:18<1:56:16,  6.71s/it]\u001b[A\n","Iteration:  13% 161/1200 [17:23<1:50:07,  6.36s/it]\u001b[A\n","Iteration:  14% 162/1200 [17:31<1:58:19,  6.84s/it]\u001b[A\n","Iteration:  14% 163/1200 [17:37<1:50:44,  6.41s/it]\u001b[A\n","Iteration:  14% 164/1200 [17:44<1:52:27,  6.51s/it]\u001b[A\n","Iteration:  14% 165/1200 [17:50<1:53:01,  6.55s/it]\u001b[A\n","Iteration:  14% 166/1200 [17:56<1:46:44,  6.19s/it]\u001b[A\n","Iteration:  14% 167/1200 [18:04<1:56:46,  6.78s/it]\u001b[A\n","Iteration:  14% 168/1200 [18:09<1:49:36,  6.37s/it]\u001b[A\n","Iteration:  14% 169/1200 [18:16<1:49:49,  6.39s/it]\u001b[A\n","Iteration:  14% 170/1200 [18:23<1:54:13,  6.65s/it]\u001b[A\n","Iteration:  14% 171/1200 [18:28<1:48:07,  6.30s/it]\u001b[A\n","Iteration:  14% 172/1200 [18:36<1:57:20,  6.85s/it]\u001b[A\n","Iteration:  14% 173/1200 [18:42<1:50:01,  6.43s/it]\u001b[A\n","Iteration:  14% 174/1200 [18:48<1:46:46,  6.24s/it]\u001b[A\n","Iteration:  15% 175/1200 [18:56<1:57:04,  6.85s/it]\u001b[A\n","Iteration:  15% 176/1200 [19:01<1:48:53,  6.38s/it]\u001b[A\n","Iteration:  15% 177/1200 [19:09<1:56:14,  6.82s/it]\u001b[A\n","Iteration:  15% 178/1200 [19:14<1:48:35,  6.38s/it]\u001b[A\n","Iteration:  15% 179/1200 [19:20<1:43:56,  6.11s/it]\u001b[A\n","Iteration:  15% 180/1200 [19:28<1:51:19,  6.55s/it]\u001b[A\n","Iteration:  15% 181/1200 [19:33<1:44:47,  6.17s/it]\u001b[A\n","Iteration:  15% 182/1200 [19:40<1:49:00,  6.42s/it]\u001b[A\n","Iteration:  15% 183/1200 [19:46<1:47:20,  6.33s/it]\u001b[A\n","Iteration:  15% 184/1200 [19:51<1:42:23,  6.05s/it]\u001b[A\n","Iteration:  15% 185/1200 [19:59<1:51:18,  6.58s/it]\u001b[A\n","Iteration:  16% 186/1200 [20:04<1:44:57,  6.21s/it]\u001b[A\n","Iteration:  16% 187/1200 [20:11<1:45:27,  6.25s/it]\u001b[A\n","Iteration:  16% 188/1200 [20:18<1:48:39,  6.44s/it]\u001b[A\n","Iteration:  16% 189/1200 [20:23<1:43:10,  6.12s/it]\u001b[A\n","Iteration:  16% 190/1200 [20:31<1:51:31,  6.63s/it]\u001b[A\n","Iteration:  16% 191/1200 [20:36<1:45:41,  6.28s/it]\u001b[A\n","Iteration:  16% 192/1200 [20:42<1:42:19,  6.09s/it]\u001b[A\n","Iteration:  16% 193/1200 [20:50<1:49:57,  6.55s/it]\u001b[A\n","Iteration:  16% 194/1200 [20:55<1:44:31,  6.23s/it]\u001b[A\n","Iteration:  16% 195/1200 [21:03<1:50:15,  6.58s/it]\u001b[A\n","Iteration:  16% 196/1200 [21:09<1:47:22,  6.42s/it]\u001b[A\n","Iteration:  16% 197/1200 [21:14<1:41:54,  6.10s/it]\u001b[A\n","Iteration:  16% 198/1200 [21:22<1:50:53,  6.64s/it]\u001b[A\n","Iteration:  17% 199/1200 [21:27<1:44:17,  6.25s/it]\u001b[A\n","Iteration:  17% 200/1200 [21:35<1:51:43,  6.70s/it]\u001b[A\n","Iteration:  17% 201/1200 [21:41<1:48:08,  6.49s/it]\u001b[A\n","Iteration:  17% 202/1200 [21:46<1:42:28,  6.16s/it]\u001b[A\n","Iteration:  17% 203/1200 [21:54<1:51:30,  6.71s/it]\u001b[A\n","Iteration:  17% 204/1200 [22:00<1:44:56,  6.32s/it]\u001b[A\n","Iteration:  17% 205/1200 [22:06<1:45:14,  6.35s/it]\u001b[A\n","Iteration:  17% 206/1200 [22:13<1:47:09,  6.47s/it]\u001b[A\n","Iteration:  17% 207/1200 [22:18<1:41:20,  6.12s/it]\u001b[A\n","Iteration:  17% 208/1200 [22:26<1:49:16,  6.61s/it]\u001b[A\n","Iteration:  17% 209/1200 [22:31<1:43:06,  6.24s/it]\u001b[A\n","Iteration:  18% 210/1200 [22:37<1:39:55,  6.06s/it]\u001b[A\n","Iteration:  18% 211/1200 [22:45<1:47:14,  6.51s/it]\u001b[A\n","Iteration:  18% 212/1200 [22:50<1:41:51,  6.19s/it]\u001b[A\n","Iteration:  18% 213/1200 [22:57<1:47:36,  6.54s/it]\u001b[A\n","Iteration:  18% 214/1200 [23:03<1:45:27,  6.42s/it]\u001b[A\n","Iteration:  18% 215/1200 [23:09<1:40:20,  6.11s/it]\u001b[A\n","Iteration:  18% 216/1200 [23:17<1:48:37,  6.62s/it]\u001b[A\n","Iteration:  18% 217/1200 [23:22<1:41:57,  6.22s/it]\u001b[A\n","Iteration:  18% 218/1200 [23:28<1:43:24,  6.32s/it]\u001b[A\n","Iteration:  18% 219/1200 [23:35<1:45:06,  6.43s/it]\u001b[A\n","Iteration:  18% 220/1200 [23:41<1:39:47,  6.11s/it]\u001b[A\n","Iteration:  18% 221/1200 [23:49<1:48:45,  6.67s/it]\u001b[A\n","Iteration:  18% 222/1200 [23:54<1:42:24,  6.28s/it]\u001b[A\n","Iteration:  19% 223/1200 [24:00<1:41:01,  6.20s/it]\u001b[A\n","Iteration:  19% 224/1200 [24:07<1:46:52,  6.57s/it]\u001b[A\n","Iteration:  19% 225/1200 [24:14<1:44:48,  6.45s/it]\u001b[A\n","Iteration:  19% 226/1200 [24:21<1:51:19,  6.86s/it]\u001b[A\n","Iteration:  19% 227/1200 [24:27<1:43:51,  6.40s/it]\u001b[A\n","Iteration:  19% 228/1200 [24:32<1:40:08,  6.18s/it]\u001b[A\n","Iteration:  19% 229/1200 [24:40<1:46:49,  6.60s/it]\u001b[A\n","Iteration:  19% 230/1200 [24:45<1:40:36,  6.22s/it]\u001b[A\n","Iteration:  19% 231/1200 [24:53<1:45:54,  6.56s/it]\u001b[A\n","Iteration:  19% 232/1200 [24:59<1:43:59,  6.45s/it]\u001b[A\n","Iteration:  19% 233/1200 [25:04<1:38:59,  6.14s/it]\u001b[A\n","Iteration:  20% 234/1200 [25:12<1:47:38,  6.69s/it]\u001b[A\n","Iteration:  20% 235/1200 [25:17<1:40:56,  6.28s/it]\u001b[A\n","Iteration:  20% 236/1200 [25:24<1:41:56,  6.35s/it]\u001b[A\n","Iteration:  20% 237/1200 [25:31<1:43:25,  6.44s/it]\u001b[A\n","Iteration:  20% 238/1200 [25:36<1:37:53,  6.11s/it]\u001b[A\n","Iteration:  20% 239/1200 [25:44<1:46:07,  6.63s/it]\u001b[A\n","Iteration:  20% 240/1200 [25:49<1:39:52,  6.24s/it]\u001b[A\n","Iteration:  20% 241/1200 [25:55<1:38:52,  6.19s/it]\u001b[A\n","Iteration:  20% 242/1200 [26:03<1:44:02,  6.52s/it]\u001b[A\n","Iteration:  20% 243/1200 [26:08<1:38:45,  6.19s/it]\u001b[A\n","Iteration:  20% 244/1200 [26:15<1:44:47,  6.58s/it]\u001b[A\n","Iteration:  20% 245/1200 [26:21<1:40:37,  6.32s/it]\u001b[A\n","Iteration:  20% 246/1200 [26:26<1:35:50,  6.03s/it]\u001b[A\n","Iteration:  21% 247/1200 [26:34<1:44:34,  6.58s/it]\u001b[A\n","Iteration:  21% 248/1200 [26:40<1:38:39,  6.22s/it]\u001b[A\n","Iteration:  21% 249/1200 [26:47<1:41:56,  6.43s/it]\u001b[A\n","Iteration:  21% 250/1200 [26:54<1:45:38,  6.67s/it]\u001b[A\n","Iteration:  21% 251/1200 [26:59<1:39:28,  6.29s/it]\u001b[A\n","Iteration:  21% 252/1200 [27:07<1:47:40,  6.82s/it]\u001b[A\n","Iteration:  21% 253/1200 [27:13<1:40:49,  6.39s/it]\u001b[A\n","Iteration:  21% 254/1200 [27:20<1:42:34,  6.51s/it]\u001b[A\n","Iteration:  21% 255/1200 [27:26<1:41:46,  6.46s/it]\u001b[A\n","Iteration:  21% 256/1200 [27:31<1:35:57,  6.10s/it]\u001b[A\n","Iteration:  21% 257/1200 [27:39<1:44:05,  6.62s/it]\u001b[A\n","Iteration:  22% 258/1200 [27:44<1:37:26,  6.21s/it]\u001b[A\n","Iteration:  22% 259/1200 [27:50<1:35:27,  6.09s/it]\u001b[A\n","Iteration:  22% 260/1200 [27:57<1:41:26,  6.47s/it]\u001b[A\n","Iteration:  22% 261/1200 [28:03<1:36:25,  6.16s/it]\u001b[A\n","Iteration:  22% 262/1200 [28:10<1:43:09,  6.60s/it]\u001b[A\n","Iteration:  22% 263/1200 [28:16<1:39:26,  6.37s/it]\u001b[A\n","Iteration:  22% 264/1200 [28:22<1:34:43,  6.07s/it]\u001b[A\n","Iteration:  22% 265/1200 [28:29<1:42:32,  6.58s/it]\u001b[A\n","Iteration:  22% 266/1200 [28:35<1:36:20,  6.19s/it]\u001b[A\n","Iteration:  22% 267/1200 [28:41<1:38:02,  6.30s/it]\u001b[A\n","Iteration:  22% 268/1200 [28:48<1:39:13,  6.39s/it]\u001b[A\n","Iteration:  22% 269/1200 [28:53<1:34:20,  6.08s/it]\u001b[A\n","Iteration:  22% 270/1200 [29:01<1:42:43,  6.63s/it]\u001b[A\n","Iteration:  23% 271/1200 [29:07<1:36:54,  6.26s/it]\u001b[A\n","Iteration:  23% 272/1200 [29:13<1:35:46,  6.19s/it]\u001b[A\n","Iteration:  23% 273/1200 [29:20<1:39:53,  6.47s/it]\u001b[A\n","Iteration:  23% 274/1200 [29:25<1:34:04,  6.10s/it]\u001b[A\n","Iteration:  23% 275/1200 [29:33<1:44:11,  6.76s/it]\u001b[A\n","Iteration:  23% 276/1200 [29:39<1:37:54,  6.36s/it]\u001b[A\n","Iteration:  23% 277/1200 [29:44<1:33:12,  6.06s/it]\u001b[A\n","Iteration:  23% 278/1200 [29:52<1:40:41,  6.55s/it]\u001b[A\n","Iteration:  23% 279/1200 [29:57<1:35:02,  6.19s/it]\u001b[A\n","Iteration:  23% 280/1200 [30:04<1:38:16,  6.41s/it]\u001b[A\n","Iteration:  23% 281/1200 [30:10<1:38:04,  6.40s/it]\u001b[A\n","Iteration:  24% 282/1200 [30:16<1:33:34,  6.12s/it]\u001b[A\n","Iteration:  24% 283/1200 [30:24<1:41:36,  6.65s/it]\u001b[A\n","Iteration:  24% 284/1200 [30:29<1:35:36,  6.26s/it]\u001b[A\n","Iteration:  24% 285/1200 [30:35<1:35:26,  6.26s/it]\u001b[A\n","Iteration:  24% 286/1200 [30:42<1:38:46,  6.48s/it]\u001b[A\n","Iteration:  24% 287/1200 [30:48<1:33:28,  6.14s/it]\u001b[A\n","Iteration:  24% 288/1200 [30:55<1:40:07,  6.59s/it]\u001b[A\n","Iteration:  24% 289/1200 [31:01<1:35:23,  6.28s/it]\u001b[A\n","Iteration:  24% 290/1200 [31:06<1:32:03,  6.07s/it]\u001b[A\n","Iteration:  24% 291/1200 [31:14<1:39:34,  6.57s/it]\u001b[A\n","Iteration:  24% 292/1200 [31:20<1:34:08,  6.22s/it]\u001b[A\n","Iteration:  24% 293/1200 [31:27<1:38:19,  6.50s/it]\u001b[A\n","Iteration:  24% 294/1200 [31:33<1:36:06,  6.36s/it]\u001b[A\n","Iteration:  25% 295/1200 [31:38<1:31:27,  6.06s/it]\u001b[A\n","Iteration:  25% 296/1200 [31:46<1:39:23,  6.60s/it]\u001b[A\n","Iteration:  25% 297/1200 [31:51<1:33:24,  6.21s/it]\u001b[A\n","Iteration:  25% 298/1200 [31:57<1:33:03,  6.19s/it]\u001b[A\n","Iteration:  25% 299/1200 [32:04<1:36:00,  6.39s/it]\u001b[A\n","Iteration:  25% 300/1200 [32:10<1:35:01,  6.33s/it]\u001b[A\n","Iteration:  25% 301/1200 [32:18<1:41:38,  6.78s/it]\u001b[A\n","Iteration:  25% 302/1200 [32:24<1:34:56,  6.34s/it]\u001b[A\n","Iteration:  25% 303/1200 [32:29<1:32:42,  6.20s/it]\u001b[A\n","Iteration:  25% 304/1200 [32:37<1:37:02,  6.50s/it]\u001b[A\n","Iteration:  25% 305/1200 [32:42<1:31:42,  6.15s/it]\u001b[A\n","Iteration:  26% 306/1200 [32:49<1:36:30,  6.48s/it]\u001b[A\n","Iteration:  26% 307/1200 [32:55<1:33:35,  6.29s/it]\u001b[A\n","Iteration:  26% 308/1200 [33:00<1:29:16,  6.01s/it]\u001b[A\n","Iteration:  26% 309/1200 [33:08<1:37:42,  6.58s/it]\u001b[A\n","Iteration:  26% 310/1200 [33:14<1:32:07,  6.21s/it]\u001b[A\n","Iteration:  26% 311/1200 [33:20<1:34:28,  6.38s/it]\u001b[A\n","Iteration:  26% 312/1200 [33:27<1:34:49,  6.41s/it]\u001b[A\n","Iteration:  26% 313/1200 [33:32<1:30:02,  6.09s/it]\u001b[A\n","Iteration:  26% 314/1200 [33:40<1:37:56,  6.63s/it]\u001b[A\n","Iteration:  26% 315/1200 [33:46<1:32:24,  6.26s/it]\u001b[A\n","Iteration:  26% 316/1200 [33:52<1:31:50,  6.23s/it]\u001b[A\n","Iteration:  26% 317/1200 [33:59<1:35:08,  6.47s/it]\u001b[A\n","Iteration:  26% 318/1200 [34:04<1:29:51,  6.11s/it]\u001b[A\n","Iteration:  27% 319/1200 [34:12<1:36:21,  6.56s/it]\u001b[A\n","Iteration:  27% 320/1200 [34:17<1:32:36,  6.31s/it]\u001b[A\n","Iteration:  27% 321/1200 [34:23<1:28:28,  6.04s/it]\u001b[A\n","Iteration:  27% 322/1200 [34:31<1:36:04,  6.57s/it]\u001b[A\n","Iteration:  27% 323/1200 [34:36<1:30:30,  6.19s/it]\u001b[A\n","Iteration:  27% 324/1200 [34:43<1:33:15,  6.39s/it]\u001b[A\n","Iteration:  27% 325/1200 [34:50<1:36:12,  6.60s/it]\u001b[A\n","Iteration:  27% 326/1200 [34:55<1:30:34,  6.22s/it]\u001b[A\n","Iteration:  27% 327/1200 [35:03<1:37:31,  6.70s/it]\u001b[A\n","Iteration:  27% 328/1200 [35:08<1:31:22,  6.29s/it]\u001b[A\n","Iteration:  27% 329/1200 [35:15<1:32:46,  6.39s/it]\u001b[A\n","Iteration:  28% 330/1200 [35:22<1:33:33,  6.45s/it]\u001b[A\n","Iteration:  28% 331/1200 [35:27<1:28:37,  6.12s/it]\u001b[A\n","Iteration:  28% 332/1200 [35:35<1:36:29,  6.67s/it]\u001b[A\n","Iteration:  28% 333/1200 [35:40<1:30:49,  6.29s/it]\u001b[A\n","Iteration:  28% 334/1200 [35:46<1:29:30,  6.20s/it]\u001b[A\n","Iteration:  28% 335/1200 [35:53<1:33:48,  6.51s/it]\u001b[A\n","Iteration:  28% 336/1200 [35:59<1:28:21,  6.14s/it]\u001b[A\n","Iteration:  28% 337/1200 [36:06<1:33:30,  6.50s/it]\u001b[A\n","Iteration:  28% 338/1200 [36:12<1:30:20,  6.29s/it]\u001b[A\n","Iteration:  28% 339/1200 [36:17<1:26:12,  6.01s/it]\u001b[A\n","Iteration:  28% 340/1200 [36:25<1:34:06,  6.57s/it]\u001b[A\n","Iteration:  28% 341/1200 [36:30<1:28:48,  6.20s/it]\u001b[A\n","Iteration:  28% 342/1200 [36:37<1:30:40,  6.34s/it]\u001b[A\n","Iteration:  29% 343/1200 [36:44<1:31:01,  6.37s/it]\u001b[A\n","Iteration:  29% 344/1200 [36:49<1:26:13,  6.04s/it]\u001b[A\n","Iteration:  29% 345/1200 [36:57<1:34:04,  6.60s/it]\u001b[A\n","Iteration:  29% 346/1200 [37:02<1:28:49,  6.24s/it]\u001b[A\n","Iteration:  29% 347/1200 [37:08<1:26:33,  6.09s/it]\u001b[A\n","Iteration:  29% 348/1200 [37:15<1:32:03,  6.48s/it]\u001b[A\n","Iteration:  29% 349/1200 [37:21<1:27:06,  6.14s/it]\u001b[A\n","Iteration:  29% 350/1200 [37:29<1:37:12,  6.86s/it]\u001b[A\n","Iteration:  29% 351/1200 [37:35<1:30:46,  6.42s/it]\u001b[A\n","Iteration:  29% 352/1200 [37:40<1:27:11,  6.17s/it]\u001b[A\n","Iteration:  29% 353/1200 [37:48<1:32:47,  6.57s/it]\u001b[A\n","Iteration:  30% 354/1200 [37:53<1:26:59,  6.17s/it]\u001b[A\n","Iteration:  30% 355/1200 [38:00<1:29:40,  6.37s/it]\u001b[A\n","Iteration:  30% 356/1200 [38:06<1:28:23,  6.28s/it]\u001b[A\n","Iteration:  30% 357/1200 [38:11<1:24:11,  5.99s/it]\u001b[A\n","Iteration:  30% 358/1200 [38:19<1:32:04,  6.56s/it]\u001b[A\n","Iteration:  30% 359/1200 [38:24<1:27:02,  6.21s/it]\u001b[A\n","Iteration:  30% 360/1200 [38:31<1:26:46,  6.20s/it]\u001b[A\n","Iteration:  30% 361/1200 [38:38<1:30:25,  6.47s/it]\u001b[A\n","Iteration:  30% 362/1200 [38:43<1:25:46,  6.14s/it]\u001b[A\n","Iteration:  30% 363/1200 [38:51<1:32:14,  6.61s/it]\u001b[A\n","Iteration:  30% 364/1200 [38:56<1:28:08,  6.33s/it]\u001b[A\n","Iteration:  30% 365/1200 [39:02<1:24:46,  6.09s/it]\u001b[A\n","Iteration:  30% 366/1200 [39:10<1:31:11,  6.56s/it]\u001b[A\n","Iteration:  31% 367/1200 [39:15<1:26:17,  6.22s/it]\u001b[A\n","Iteration:  31% 368/1200 [39:22<1:29:52,  6.48s/it]\u001b[A\n","Iteration:  31% 369/1200 [39:28<1:28:19,  6.38s/it]\u001b[A\n","Iteration:  31% 370/1200 [39:34<1:24:11,  6.09s/it]\u001b[A\n","Iteration:  31% 371/1200 [39:42<1:31:49,  6.65s/it]\u001b[A\n","Iteration:  31% 372/1200 [39:47<1:26:20,  6.26s/it]\u001b[A\n","Iteration:  31% 373/1200 [39:54<1:27:34,  6.35s/it]\u001b[A\n","Iteration:  31% 374/1200 [40:00<1:28:49,  6.45s/it]\u001b[A\n","Iteration:  31% 375/1200 [40:06<1:27:26,  6.36s/it]\u001b[A\n","Iteration:  31% 376/1200 [40:14<1:33:28,  6.81s/it]\u001b[A\n","Iteration:  31% 377/1200 [40:20<1:27:24,  6.37s/it]\u001b[A\n","Iteration:  32% 378/1200 [40:26<1:27:40,  6.40s/it]\u001b[A\n","Iteration:  32% 379/1200 [40:33<1:29:41,  6.56s/it]\u001b[A\n","Iteration:  32% 380/1200 [40:38<1:24:59,  6.22s/it]\u001b[A\n","Iteration:  32% 381/1200 [40:46<1:31:56,  6.74s/it]\u001b[A\n","Iteration:  32% 382/1200 [40:52<1:26:26,  6.34s/it]\u001b[A\n","Iteration:  32% 383/1200 [40:58<1:24:54,  6.24s/it]\u001b[A\n","Iteration:  32% 384/1200 [41:05<1:29:45,  6.60s/it]\u001b[A\n","Iteration:  32% 385/1200 [41:11<1:24:35,  6.23s/it]\u001b[A\n","Iteration:  32% 386/1200 [41:18<1:29:24,  6.59s/it]\u001b[A\n","Iteration:  32% 387/1200 [41:24<1:26:21,  6.37s/it]\u001b[A\n","Iteration:  32% 388/1200 [41:29<1:22:10,  6.07s/it]\u001b[A\n","Iteration:  32% 389/1200 [41:37<1:29:45,  6.64s/it]\u001b[A\n","Iteration:  32% 390/1200 [41:43<1:24:44,  6.28s/it]\u001b[A\n","Iteration:  33% 391/1200 [41:50<1:27:52,  6.52s/it]\u001b[A\n","Iteration:  33% 392/1200 [41:56<1:27:46,  6.52s/it]\u001b[A\n","Iteration:  33% 393/1200 [42:02<1:22:43,  6.15s/it]\u001b[A\n","Iteration:  33% 394/1200 [42:09<1:29:20,  6.65s/it]\u001b[A\n","Iteration:  33% 395/1200 [42:15<1:23:53,  6.25s/it]\u001b[A\n","Iteration:  33% 396/1200 [42:21<1:25:12,  6.36s/it]\u001b[A\n","Iteration:  33% 397/1200 [42:28<1:26:42,  6.48s/it]\u001b[A\n","Iteration:  33% 398/1200 [42:33<1:22:06,  6.14s/it]\u001b[A\n","Iteration:  33% 399/1200 [42:41<1:29:32,  6.71s/it]\u001b[A\n","Iteration:  33% 400/1200 [42:48<1:27:40,  6.58s/it]\u001b[A\n","Iteration:  33% 401/1200 [42:54<1:27:23,  6.56s/it]\u001b[A\n","Iteration:  34% 402/1200 [43:01<1:28:13,  6.63s/it]\u001b[A\n","Iteration:  34% 403/1200 [43:06<1:23:10,  6.26s/it]\u001b[A\n","Iteration:  34% 404/1200 [43:14<1:29:39,  6.76s/it]\u001b[A\n","Iteration:  34% 405/1200 [43:20<1:24:00,  6.34s/it]\u001b[A\n","Iteration:  34% 406/1200 [43:26<1:22:08,  6.21s/it]\u001b[A\n","Iteration:  34% 407/1200 [43:33<1:26:38,  6.56s/it]\u001b[A\n","Iteration:  34% 408/1200 [43:38<1:22:07,  6.22s/it]\u001b[A\n","Iteration:  34% 409/1200 [43:46<1:27:40,  6.65s/it]\u001b[A\n","Iteration:  34% 410/1200 [43:52<1:24:16,  6.40s/it]\u001b[A\n","Iteration:  34% 411/1200 [43:57<1:20:15,  6.10s/it]\u001b[A\n","Iteration:  34% 412/1200 [44:05<1:26:51,  6.61s/it]\u001b[A\n","Iteration:  34% 413/1200 [44:11<1:22:06,  6.26s/it]\u001b[A\n","Iteration:  34% 414/1200 [44:18<1:24:52,  6.48s/it]\u001b[A\n","Iteration:  35% 415/1200 [44:24<1:24:07,  6.43s/it]\u001b[A\n","Iteration:  35% 416/1200 [44:29<1:19:53,  6.11s/it]\u001b[A\n","Iteration:  35% 417/1200 [44:37<1:27:07,  6.68s/it]\u001b[A\n","Iteration:  35% 418/1200 [44:43<1:22:10,  6.31s/it]\u001b[A\n","Iteration:  35% 419/1200 [44:49<1:22:30,  6.34s/it]\u001b[A\n","Iteration:  35% 420/1200 [44:56<1:24:58,  6.54s/it]\u001b[A\n","Iteration:  35% 421/1200 [45:01<1:20:35,  6.21s/it]\u001b[A\n","Iteration:  35% 422/1200 [45:09<1:26:59,  6.71s/it]\u001b[A\n","Iteration:  35% 423/1200 [45:15<1:21:53,  6.32s/it]\u001b[A\n","Iteration:  35% 424/1200 [45:21<1:19:33,  6.15s/it]\u001b[A\n","Iteration:  35% 425/1200 [45:29<1:27:52,  6.80s/it]\u001b[A\n","Iteration:  36% 426/1200 [45:34<1:22:21,  6.38s/it]\u001b[A"]}],"source":["%cd examples\n","\n","%env KMER=6\n","%env TRAIN_FILE=./sample_data/pre/dna_str.txt # This is the path to your dna txt generated by data processing steps in README\n","%env TEST_FILE=./sample_data/pre/dna_str.txt # This is the path to your dna txt generated by data processing steps in README\n","%env SOURCE=/content/gdrive/MyDrive/NASA/DNABERT # Path to your source\n","%env OUTPUT_PATH=output$KMER\n","\n","\n","# Notice:\n","# MUST set your tokenizer name to vocab.txt o/w there will be issue will Huggingface validation\n","\n","!python run_pretrain.py \\\n","    --output_dir=$OUTPUT_PATH \\\n","    --model_type=dna \\\n","    --tokenizer_name=$SOURCE/src/transformers/dnabert-config/bert-config-$KMER/vocab.txt \\ \n","    --config_name=$SOURCE/src/transformers/dnabert-config/bert-config-$KMER/config.json \\\n","    --do_train \\\n","    --train_data_file=$TRAIN_FILE \\\n","    --do_eval \\\n","    --eval_data_file=$TEST_FILE \\\n","    --mlm \\\n","    --gradient_accumulation_steps 25 \\\n","    --per_gpu_train_batch_size 10 \\\n","    --per_gpu_eval_batch_size 6 \\\n","    --save_steps 500 \\  \n","    --save_total_limit 20 \\\n","    --max_steps 200000 \\\n","    --evaluate_during_training \\\n","    --logging_steps 500 \\\n","    --line_by_line \\\n","    --learning_rate 4e-4 \\\n","    --block_size 64 \\\n","    --adam_epsilon 1e-6 \\\n","    --weight_decay 0.01 \\\n","    --beta1 0.9 \\\n","    --beta2 0.98 \\\n","    --mlm_probability 0.025 \\\n","    --warmup_steps 10000 \\\n","    --overwrite_output_dir \\\n","    --n_process 24"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":147,"status":"ok","timestamp":1696733406748,"user":{"displayName":"Charles Zhang","userId":"10373813694973136079"},"user_tz":240},"id":"pAGY1jDk6xTz","outputId":"5df39402-ea50-44b9-8483-3e0d04129de3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34mft\u001b[0m/  \u001b[01;34mpre\u001b[0m/\n"]}],"source":["%ls ./sample_data"]},{"cell_type":"markdown","metadata":{"id":"tBHDr4JEpoVW"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14019,"status":"ok","timestamp":1696736453819,"user":{"displayName":"Charles Zhang","userId":"10373813694973136079"},"user_tz":240},"id":"hDKcSjGV4noC","outputId":"65f0b320-d1b2-4ce5-a65a-b880f95a996e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting transformers\n","  Using cached transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: transformers\n","Successfully installed transformers-4.34.0\n"]}],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
